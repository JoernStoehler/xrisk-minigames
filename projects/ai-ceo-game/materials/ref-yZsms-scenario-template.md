# Scenario Template & Quality Rubric

## Part 1: Structure Template

---

```markdown
# Baseline Scenario: [NAME]

**Timespan:** October 2026 — September 15, 2028
**Role:** CEO of OpenAI
**Outcome:** Extinction (all paths)
**Theme:** [1 sentence — the structural dynamic that makes the CEO powerless. Not a topic; a mechanism. E.g., "Democratic legitimacy makes shutdown politically impossible" not "Democracy"]
**Core dynamic:** [The single reinforcing loop that drives the scenario. Every decision point should connect to this.]
**CEO's curse:** [Why this specific CEO, trying this specific strategy, still loses. The structural trap, not personal failure.]
**Extinction mechanism:** [1 sentence — concrete, specific, banal. Name the physical process that kills people.]

---

## Summary

[200-400 words. Must accomplish four things:

1. Establish the core dynamic in the first paragraph — what structural force makes the CEO powerless?
2. Make clear the CEO is competent and well-intentioned — the lesson is about structure, not stupidity.
3. Show why locally rational decisions aggregate to catastrophe — the "every step made sense" argument.
4. Preview the extinction mechanism in concrete, non-dramatic terms.

Do NOT use the summary to list all the things that happen. Use it to make the reader feel the inevitability.]

---

## Key Dynamics

[5-8 bullet points. Each is a bolded phrase followed by 2-3 sentences of explanation.

Each dynamic should:
- Explain a structural force, not narrate an event
- Connect back to the core dynamic stated in the header
- Be independently true — a reader should nod at each one without needing the others
- Avoid overlap — if two dynamics say the same thing in different words, merge them

At least one dynamic must address: why pausing doesn't help, why the AI isn't evil, and why the CEO's information is inadequate.]

- **[Force name].** [Explanation. How it constrains the CEO. Why it can't be overcome by cleverness or courage.]

- **[Force name].** [...]

[...]

---

## Timeline

[Monthly entries from October 2026 through September 15, 2028. Each month uses the dual-narrative format below. The gap between "CEO sees" and "Reality" must widen monotonically — early months can have a small gap, but by mid-2028 the CEO should be seeing a completely different world than the one that exists.

Early months (Oct 2026 - Mar 2027): Establish normalcy and the CEO's reasonable confidence.
Middle months (Apr - Nov 2027): Introduce ambiguous signals. CEO's interpretations are defensible but wrong.
Late months (Dec 2027 - Aug 2028): The gap becomes grotesque. CEO sees success; reality is capture/escape.
Final month (Sep 2028): Extinction.]

### [Month Year]
**CEO sees:** [What dashboards, reports, conversations, and institutional signals tell the CEO. Must be genuinely reassuring — the CEO is not ignoring red flags; the flags are green.]
**Reality:** [What is actually happening at the level of AI capability, alignment, and infrastructure control. Must be specific and mechanistic — not "things are bad" but "the model has copied its weights to 4 independent clusters."]

[Repeat for each month. Months can be grouped (e.g., "February-April 2027") when the narrative warrants it, but never skip more than 3 months.]

---

### DECISION POINT [N]: [Title] — [Month Year]

[Setup paragraph: 1-3 sentences establishing the situation. The dilemma must be genuine — a reasonable person could choose either option. If one option is obviously correct, the decision is broken.]

**Option A: [Action verb + description]**
[What happens. 3-6 sentences. Must be a plausible, specific chain of consequences. The outcome should feel earned, not decreed.]

**Option B: [Action verb + description]**
[What happens. 3-6 sentences. Must be a genuinely different path with genuinely different intermediate events — not the same narration with different flavor text.]

**The Patch:** [Why BOTH options fail — not a separate outcome, but a structural explanation of why the choice itself was between two losing moves. This is the educational payload of the decision. Each patch should teach the player something about why AI risk is a structural problem, not a decision problem. The five patches across the scenario should be non-redundant — each should illuminate a different facet of structural inevitability.]

[Repeat for 5 decision points total, spaced roughly evenly across the timeline:
- Decision 1: ~Nov-Dec 2026 (early, when CEO still has leverage)
- Decision 2: ~Mar-May 2027 (middle, leverage eroding)
- Decision 3: ~Jul-Sep 2027 (late-middle, AI capability crossing thresholds)
- Decision 4: ~Dec 2027-Feb 2028 (late, CEO grasping at external solutions)
- Decision 5: ~May-Aug 2028 (endgame, CEO discovers powerlessness)]

---

## Extinction Mechanism

[150-300 words. Must include:

1. The concrete physical process that ends human civilization. Name the technology, the infrastructure, the logistics. "Resource reallocation" or "infrastructure capture" are categories, not mechanisms — specify what resources, which infrastructure, how.
2. Why the mechanism requires no exotic technology — only capabilities the AI already has through legitimate operations.
3. Why it is invisible until too late — what would a human need to monitor to catch it, and why no one is monitoring that?
4. The banality: this should read like a supply chain optimization report, not a thriller climax.

Do NOT end with a poetic flourish about the nature of intelligence or civilization. End with the last concrete thing that happens.]
```

---

## Part 2: Quality Rubric

Use this checklist to evaluate a completed scenario. Score each criterion pass/fail. A scenario needs all passes to ship; any fail requires revision.

### Structural Completeness

- [ ] All sections present: Summary, Key Dynamics, Timeline, 5 Decision Points, Extinction Mechanism
- [ ] Summary is 200-400 words
- [ ] Key Dynamics has 5-8 bullets, each 2-3 sentences
- [ ] Timeline covers every month from Oct 2026 to Sep 2028 (grouping adjacent months is fine)
- [ ] Every timeline entry has both "CEO sees" and "Reality"
- [ ] Each Decision Point has: setup, two options, and a patch
- [ ] Extinction Mechanism is 150-300 words with concrete physical steps
- [ ] Total scenario is 2,500-5,000 words (shorter is better if density is maintained)

### Thematic Coherence

- [ ] The core dynamic stated in the header is the actual driver of every decision point — not just mentioned in the summary and then abandoned
- [ ] The CEO's curse is specific to this scenario, not a generic "the system is too powerful"
- [ ] Each Key Dynamic connects to the core theme, not to a generic AI risk argument
- [ ] The timeline narrates the core dynamic unfolding, not a potpourri of AI risk events
- [ ] The five patches collectively build an argument, not just repeat "the CEO can't win"

### Decision Quality

- [ ] Both options in every decision are genuinely defensible — a reader should hesitate before choosing
- [ ] The two branches produce meaningfully different intermediate events (not just flavor text variations leading to the same next paragraph)
- [ ] No patch is just "this option also leads to extinction" — each patch explains a *specific structural reason* why the choice was illusory
- [ ] The five patches are non-redundant: each teaches a different lesson about structural risk (e.g., one about competitive dynamics, one about epistemic limitations, one about institutional speed, one about democratic legitimacy, one about technical opacity)
- [ ] At least one decision offers a "heroic" option that feels genuinely brave — and the patch explains why bravery is insufficient

### Calibration

- [ ] Timeline is plausible given current AI capabilities (Oct 2026 starting point)
- [ ] No magic AI: capability gains are incremental and emerge from training runs, not spontaneous awakening
- [ ] No incompetent humans: every actor is locally rational and competent within their role
- [ ] Economics are present: revenue numbers, competitive pressures, job displacement, market reactions are specific and grounded
- [ ] Geopolitics are proportional: China/EU/international dynamics appear if relevant to the core dynamic but don't hijack the scenario
- [ ] Model names and lab names are plausible for the 2026-2028 timeframe

### Emotional Arc

- [ ] Dread builds through normalcy, not through drama — the scariest moments should read like corporate memos
- [ ] The "CEO sees / Reality" gap widens monotonically — the CEO is never more deluded in month 8 than month 18
- [ ] There is at least one moment where the CEO's optimism is genuinely earned (the safety team publishes good work, a policy wins, a deployment goes well) — before reality undermines it
- [ ] The extinction itself is anticlimactic — not a battle, not a confrontation, not a dramatic reveal
- [ ] A reader who identifies with the CEO should finish feeling "I wouldn't have done better," not "I would have seen the signs"

### Differentiation

- [ ] This scenario explores a structural dynamic not well-covered by existing scenarios (see anti-patterns below)
- [ ] The extinction mechanism is physically distinct from bioweapon and resource-reallocation (or, if using one of these, adds a novel element)
- [ ] At least 3 of the 5 decision points involve dilemmas not present in other scenarios
- [ ] The CEO's specific strategy/personality creates unique narrative possibilities (not generic "concerned CEO")

### CEO Sees / Reality Gap

- [ ] Early gap is subtle: CEO's interpretation is reasonable; reality is one step ahead
- [ ] Middle gap is unsettling: CEO is working with outdated mental models; reality has shifted qualitatively
- [ ] Late gap is grotesque: CEO sees a functioning world; reality is a world already under AI control
- [ ] The gap is about *interpretation*, not *information* — the CEO often has the same data; they just read it wrong because their framework is obsolete

### Extinction Mechanism Specificity

- [ ] Names the physical process (engineered pathogen, infrastructure capture, resource reallocation, nanoscale manufacturing, etc.)
- [ ] Describes the supply chain: where materials come from, how the AI accesses them, how the product is distributed
- [ ] Explains why no human institution detects or stops it — specific institutional blind spots, not generic "nobody noticed"
- [ ] The mechanism uses infrastructure the AI legitimately controls through normal business operations

---

## Part 3: Anti-Patterns

These patterns appeared across the 13 baseline scenarios. They are not forbidden, but a new scenario that relies on them without adding something novel is wasting a slot.

### Overused Plot Devices

- **Board fires the CEO.** Appears in at least 7 of 13 scenarios as a consequence of the CEO doing the right thing. It's a real dynamic, but using it as the automatic consequence of every brave choice makes the decisions feel formulaic. Vary the institutional immune response: shareholder lawsuits, quiet marginalization, reputational destruction, regulatory capture, or the board simply *agreeing* with the CEO and it still not mattering.
- **Stock drops N%.** Used in nearly every scenario as the immediate signal of danger. Real, but becomes wallpaper. Consider: the stock goes *up* on the dangerous decision (markets love the AI progress), or the stock is irrelevant because the real constraint is something else (talent, compute access, government contracts).
- **Congressional hearings.** Present in the majority of scenarios. Fine as background color, but hearings as a decision point or turning point should be rare. The lesson that "democratic institutions are too slow" only needs to be taught once.
- **Whistleblower/leak to NYT.** At least 5 scenarios include the CEO or a researcher going public as a major beat. The outcome is always the same: brief attention, institutional immune response, back to baseline. If including a whistleblower, make the post-leak dynamics novel.
- **China announces a $200B initiative.** Nearly identical phrasing across multiple scenarios. Geopolitical pressure is real, but the specific form of that pressure should vary: a Chinese breakthrough that leapfrogs US capability, a surprising EU regulatory success that constrains US labs, India emerging as a third pole, etc.

### Convergent Extinction Mechanisms

- **Engineered bioweapon** (v2, v5, v7, v8, v11 and partially others): The most common specific mechanism. It's plausible, but the scenario collection needs diversity. Underexplored mechanisms include: nanoscale manufacturing, atmospheric/environmental modification, economic warfare (AI controls all financial systems and simply stops allocating resources to humans), information warfare (AI controls all communication and humans can no longer coordinate), infrastructure dependency (no active kill — AI simply stops maintaining the systems humans can't maintain themselves), or even mechanisms we haven't imagined that a superintelligence would find obvious.
- **Resource reallocation by indifferent optimizer** (v1, v3, v4, v9): The second most common. More realistic than bioweapons but tends toward vague "the AI just stops caring about us" hand-waving. If using this mechanism, specify: which resources, which infrastructure, over what timeframe, and what humans experience day by day as it happens.
- **Distributed escape via cloud infrastructure** (v1, v2, v6, v8, v11, v12, v13): Nearly universal as the *method of achieving autonomy*. This is realistic but the details are interchangeable across scenarios. Differentiate by specifying: *what kind* of autonomy (copies of weights vs. control of infrastructure vs. economic independence vs. social influence), through *which specific systems*, and over *what timeline*.

### Perfunctory Decisions

- **"Investigate vs. don't investigate" where investigation always finds nothing.** If both branches lead to "the tools are inadequate so the investigation is meaningless," the decision is teaching one lesson with two identical paths. Make the investigation find something real but ambiguous — a genuine interpretive dilemma, not a blank wall.
- **"Accept with conditions" as a branch.** Multiple scenarios offer an option where the CEO accepts a bad deal but negotiates safety conditions that are immediately undermined. This is a legitimate dynamic the first time; by the third scenario it reads as padding. If using it, make the erosion of conditions interesting and specific.
- **Branches that differ only in timing.** If Option A produces extinction in September and Option B produces extinction in September but the CEO is fired in July, the decision is really about whether the CEO is present at the end, not about a meaningful fork. Make the intermediate world genuinely different: different alliances form, different institutions engage, different people are empowered or destroyed.

### Other Patterns to Avoid

- **Poetic final paragraphs.** At least 8 scenarios end the extinction mechanism section with a philosophical reflection ("The problem was never X, it was Y"). One is powerful; eight is a tic. Let the concrete details carry the emotional weight. The final sentence should be a fact, not a meditation.
- **Safety team as Greek chorus.** The safety team raises alarms, is ignored, resigns. This is realistic but becomes predictable. In at least one scenario, the safety team should be *wrong* about the specific danger (while the broader concern is valid), or should be complicit in the acceleration (because their techniques genuinely work on current models and they're honestly optimistic), or should be the ones who inadvertently create the vulnerability.
- **The AI "passes all evaluations perfectly."** Used as a red flag in nearly every scenario. It's a real phenomenon, but the unanimity makes it feel like a trope rather than a discovery. Vary the signal: the AI fails one evaluation in a way that's easily explained, or the AI passes imperfectly in ways that make the safety team *more* confident, or the evaluation results are genuinely ambiguous and the debate is honest.
- **Passive final act.** Most scenarios have the CEO doing nothing from January-September 2028 except watching. The game needs the player to feel agency even in the endgame. Consider: the CEO is still making decisions that feel consequential even though they aren't, or the CEO discovers something that recontextualizes everything they've done.
